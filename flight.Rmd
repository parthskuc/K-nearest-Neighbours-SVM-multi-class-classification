---
title: Multi class classification using Logistic Regression, K nearest Neighbors and SVM
  SVM
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

## Flight Accident Severity Prediction.
##According to the FAA, 2,781,971 passengers fly every day in the US, as in June 2019. Passengers reckon that flying is very safe, considering strict inspections are conducted and security measures are taken to avoid and/or mitigate any mishappenings. However, there remain a few chances of unfortunate incidents.


## Loading Packages required
```{r}
library(ISLR)
library(tree)
library(ggplot2)
library(tidyverse)
library(MASS)
require(randomForest)
library(psych)
library(corrplot)
library(caret)
library(nnet)
```
##The dataset consists of certain parameters recorded during the incident` such as cabin temperature, turbulence experienced, number of safety complaints prior to the accident, number of days since the last inspection was conducted before the incident, an estimation of the pilotâ€™s control given the various factors at play

### Loading our training dataset
```{r}
df<-read.csv("train.csv")
head(df)

```
### Loading our testing dataset. We'll use this later.
```{r}
df2<-read.csv("test.csv")
head(df2)
```

### Checking the size of our data
```{r}
dim(df)
```
### Looking for missing values
```{r}
colSums(is.na(df))
```
## Checking for duplicate values in our dataset
```{r}
df[!duplicated(df[1:12]),]
```
### Plotting the distributions of numeric columns 
```{r}
ggplot(df,aes(x=df$Safety_Score))+geom_histogram(aes(y=..density..),color="black",fill="white",binwidth = 2)+stat_function(fun=dnorm,args = list(mean=mean(df$Safety_Score), sd=sd(df$Safety_Score)))
```


```{r}
ggplot(df,aes(x=df$Days_Since_Inspection))+geom_histogram(aes(y =..density..), color="black",fill="white",binwidth = 2)+stat_function(fun = dnorm, args = list(mean = mean(df$Days_Since_Inspection), sd = sd(df$Days_Since_Inspection)))

```
```{r}
ggplot(df,aes(x=df$Total_Safety_Complaints))+geom_histogram(color="black",fill="white",binwidth = 2)

```
```{r}
ggplot(df,aes(x=df$Control_Metric))+geom_histogram(color="black", fill="white",binwidth=1)
```
```{r}
ggplot(df,aes(x=df$Turbulence_In_gforces))+geom_histogram(color="black", fill="white")
```
```{r}
ggplot(df,aes(x=df$Cabin_Temperature))+geom_histogram(color="black", fill="white")
```
```{r}
ggplot(df,aes(x=df$Max_Elevation))+geom_histogram(color="black", fill="white")
```
```{r}
ggplot(df,aes(x=df$Violations))+geom_histogram(color="black", fill="white",binwidth=0.5)
```
```{r}
ggplot(df,aes(x=df$Adverse_Weather_Metric))+geom_histogram(color="black", fill="white")
```
###  PLotting and Mapping the correlation of the predictors
```{r}
pairs.panels(df[c("Safety_Score","Days_Since_Inspection","Total_Safety_Complaints","Control_Metric",
"Turbulence_In_gforces","Cabin_Temperature","Max_Elevation","Violations","Adverse_Weather_Metric")],hist.col="green",gap=0)
```
###Mapping the correlation
```{r}
mydf<-df[,c("Safety_Score","Days_Since_Inspection","Total_Safety_Complaints","Control_Metric",
"Turbulence_In_gforces","Cabin_Temperature","Max_Elevation","Violations","Adverse_Weather_Metric")]
res<-cor(mydf)
round(res,2)
```
```{r}
corrplot(res, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```


```{r}
df$Severity <- as.factor(df$Severity)
#df$Control_Metric <- as.factor(df$Control_Metric)
#df$Max_Elevation <- as.factor(df$Max_Elevation)
#df$Safety_Score <- as.factor(df$Safety_Score)
#df$Turbulence_In_gforces <- as.factor(df$Turbulence_In_gforces)
#df$Violations <- as.factor(df$Violations)
#df$Days_Since_Inspection <- as.factor(df$Days_Since_Inspection)
#df$Cabin_Temperature <- as.factor(df$Cabin_Temperature)
#df$Adverse_Weather_Metric <- as.factor(df$Adverse_Weather_Metric)
#df$Total_Safety_Complaints <- as.factor(df$Total_Safety_Complaints)
#df$Accident_Type_Code <- as.factor(df$Accident_Type_Code)
#df$Accident_ID <- as.factor(df$Accident_ID)
```

### Splitting our dataset into train and test sets. I am keeping the size of training data 80% of the whole data.
```{r}
df<-df[,-12]
sample_size=floor(0.8*nrow(df))
set.seed(123)
df_train= sample(seq_len(nrow(df)),size = sample_size)
train=df[df_train,]
test=df[-df_train,]
```

### MultiClassification Logistic Regression
```{r}
model1<-nnet::multinom(Severity~.,data = train)
summary(model1)

```
### Predicitng on our test data
```{r}
predicted.classes <- model1 %>% predict(test)
```

```{r}
head(predicted.classes)
```
### checking the accuracy our the model
```{r}
mean(predicted.classes == test$Severity)
```
```{r}
head(test[,1])
```
### K nearest neighbor Classification 
```{r,include=FALSE}
library(class)
knn_df <- knn(train = train[-1],test= test[-1],cl=train[,1],k=1)
knn_df
```


```{r}
table(test[,1],knn_df,dnn=c("True","Predicted"))
```
### Checking the accuracy of our model
```{r}
mean(knn_df==test[,1])
```

```{r}
miserror <- sum(test[,1]!=knn_df)/nrow(test)
miserror

```

### Training a decision tree model
```{r,include=FALSE}
library(rpart)
model3<-rpart(Severity~.,data = train,method = "class")

predict(model3,test[,-1],type="class")
```

### Plotting The decision tree 
```{r}
#install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(model3)
```
### Testing the model
```{r}
test$pred <- predict(model3,test[,-1],type="class")
```

```{r}
table(test$pred, test$Severity)
```
### Checking the accuracy
```{r}
mean(test$pred == test$Severity)
```

### Random Forest Model
```{r}
library(randomForest)
model4<-randomForest(Severity~.,data = train, xtree=400,mtry=6,nodesize=2)
```

```{r}
test$pred_rf2 <- predict(model4,test[,-1])
```

```{r}
table(test$pred_rf2, test[,1])
```
### Checking the accuracy of Random Forest
```{r}
mean(test$pred_rf2==test[,1])
```

```{r}
print(model4)
```
### Tuning the Random Forest Model using Bagging

```{r}
#install.packages("bagRboostR")
library(bagRboostR)
```


```{r}
df2$Severity <- predict(model4,df2)
```

```{r}

oob = trainControl(method = "oob")
cv_5 = trainControl(method = "cv", number = 5)
```

### Checking for the best parameters
```{r}
rf_grid =  expand.grid(mtry = 1:10)

set.seed(825)
flight_rf_tune = train(Severity ~ ., data = train,
                     method = "rf",
                     trControl = oob,
                     verbose = FALSE,
                     tuneGrid = rf_grid)
flight_rf_tune
```

### Checking the accuracy after bagging
```{r}
calc_acc = function(actual, predicted) {
  mean(actual == predicted)
}
```

```{r,include=FALSE}
library(caret)
predict(flight_rf_tune, test)

calc_acc(predict(flight_rf_tune, test), test$Severity)

```
```{r}
flight_rf_tune$bestTune
```
### Predicting our Random Forest model on test tata
```{r}
df2$Severity <- predict(flight_rf_tune,df2)
```


### Training a Support Vector Model
```{r}
set.seed(120)
model5 <- train(Severity~.,data=train,method="svmLinear", trControl = trainControl("cv", number = 10),
  preProcess = c("center","scale"))
```

```{r}
test$pred_svm <- model5 %>% predict(test[,-1])
head(test$pred_svm)
```

### Checking the accuracy 
```{r}
mean(test$pred_svm == test[,1])
```

#### We see her that the tuned Random Forest model gave us the best accuracy i.e 94.55 and we'll consider that as our final model.

### Conclusion: 
####Building this Machine Learning models can help us to anticipate and classify the severity of any airplane accident based on past incidents. With this, all airlines, even the entire aviation industry, can predict the severity of airplane accidents caused due to various factors and, correspondingly, have a plan of action to minimize the risk associated with them.


#### Further in the analysis I am planning to impove the accuracy and prediction capability by boosting the model using gradient boost.
```{r}
#gbm_grid =  expand.grid(interaction.depth = 1:5, n.trees = (1:6) * 500, shrinkage = c(0.001, 0.01, 0.1),n.minobsinnode = 10)
```

```{r}
#install.packages("gbm")
#library(gbm)
#flight_gbm_tune = train(Severity ~ ., data = train,method = "gbm", trControl = cv_5, verbose = FALSE, tuneGrid = gbm_grid)
```

